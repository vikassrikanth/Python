{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vikas.srikanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#-------------------------------------------------------------------------------------#\n",
    "#--------------------------------Numpy and Pandas-------------------------------------#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "#-------------------------------------------------------------------------------------#\n",
    "#--------------------------------NLTK------------------------------------------------#\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#-------------------------------------------------------------------------------------#\n",
    "#-----------------------------SKITLEARN----------------------------------------------#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#-------------------------------------------------------------------------------------#\n",
    "#--------------------------------Plotly------------------------------------------------#\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#-------------------------------------------------------------------------------------#\n",
    "#--------------------------------Cuflinks---------------------------------------------#\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas.srikanth\\Youtube\\Home Decor\\Comments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\vikas.srikanth\\\\Youtube\\\\Home Decor\\\\Comments\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"homeDecor_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>commentText</th>\n",
       "      <th>date</th>\n",
       "      <th>hasReplies</th>\n",
       "      <th>id</th>\n",
       "      <th>likes</th>\n",
       "      <th>numberOfReplies</th>\n",
       "      <th>replies.commentText</th>\n",
       "      <th>replies.date</th>\n",
       "      <th>replies.id</th>\n",
       "      <th>replies.likes</th>\n",
       "      <th>replies.timestamp</th>\n",
       "      <th>replies.user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi Guys Thanks For your supports please watch ...</td>\n",
       "      <td>1 year ago</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyTEohZImUtWa8JPzB4AaABAg</td>\n",
       "      <td>53.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.531420e+12</td>\n",
       "      <td>DIY Crafts &amp; Lifehacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIY Crafts &amp; Lifehacks \\nCan u plz told how ma...</td>\n",
       "      <td>1 year ago</td>\n",
       "      <td>UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8dtQT1j_HL5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.531420e+12</td>\n",
       "      <td>M.riaz123 M.riaz123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIY Crafts &amp; Lifehacks</td>\n",
       "      <td>1 year ago</td>\n",
       "      <td>UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8e3TmP-iVvj</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.531420e+12</td>\n",
       "      <td>Raz kumar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plzzzzz? ??\\nI want to make it. .BT due to not...</td>\n",
       "      <td>1 year ago</td>\n",
       "      <td>UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8eApw5z84pT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.531420e+12</td>\n",
       "      <td>uzma riaz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DIY Crafts &amp; Lifehacks</td>\n",
       "      <td>1 year ago</td>\n",
       "      <td>UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8g64spqz3xu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.531420e+12</td>\n",
       "      <td>Anik mukherjee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        commentText        date  \\\n",
       "0           0  Hi Guys Thanks For your supports please watch ...  1 year ago   \n",
       "1           1                                                NaN         NaN   \n",
       "2           2                                                NaN         NaN   \n",
       "3           3                                                NaN         NaN   \n",
       "4           4                                                NaN         NaN   \n",
       "\n",
       "  hasReplies                          id  likes  numberOfReplies  \\\n",
       "0       True  UgyTEohZImUtWa8JPzB4AaABAg   53.0              7.0   \n",
       "1        NaN                         NaN    NaN              NaN   \n",
       "2        NaN                         NaN    NaN              NaN   \n",
       "3        NaN                         NaN    NaN              NaN   \n",
       "4        NaN                         NaN    NaN              NaN   \n",
       "\n",
       "                                 replies.commentText replies.date  \\\n",
       "0                                                NaN          NaN   \n",
       "1  DIY Crafts & Lifehacks \\nCan u plz told how ma...   1 year ago   \n",
       "2                             DIY Crafts & Lifehacks   1 year ago   \n",
       "3  Plzzzzz? ??\\nI want to make it. .BT due to not...   1 year ago   \n",
       "4                             DIY Crafts & Lifehacks   1 year ago   \n",
       "\n",
       "                                          replies.id  replies.likes  \\\n",
       "0                                                NaN            NaN   \n",
       "1  UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8dtQT1j_HL5            0.0   \n",
       "2  UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8e3TmP-iVvj            0.0   \n",
       "3  UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8eApw5z84pT            0.0   \n",
       "4  UgyTEohZImUtWa8JPzB4AaABAg.8dEAZ8meOff8g64spqz3xu            0.0   \n",
       "\n",
       "   replies.timestamp         replies.user     timestamp  \\\n",
       "0                NaN                  NaN  1.531420e+12   \n",
       "1       1.531420e+12  M.riaz123 M.riaz123           NaN   \n",
       "2       1.531420e+12            Raz kumar           NaN   \n",
       "3       1.531420e+12            uzma riaz           NaN   \n",
       "4       1.531420e+12       Anik mukherjee           NaN   \n",
       "\n",
       "                     user  \n",
       "0  DIY Crafts & Lifehacks  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_eng = stopwords.words('english')\n",
    "customstopwords =[\"amzn\",\"http\", \"https\", \"with\", \"this\", \"that\"]\n",
    "#def function(tokens):\n",
    "   # return word_tokenize([w for w in tokens if not w in stop_eng and not w in customstopwords and w.isalpha() and not len(w)<3 and not w in hashtags and not w in ghashtags and not w in links and not w in mentions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringMethods' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9d2402a0d136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# remove short words (length < 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'commentText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'commentText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# remove stopwords from the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StringMethods' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# function to remove stopwords\n",
    "\n",
    "dataset['commentText']=dataset['commentText'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "def remove_stopwords(rev):\n",
    "    customstopwords =[\"amzn\",\"http\", \"https\", \"with\", \"this\", \"that\"]\n",
    "    rev_new = \" \".join([i for i in rev if i not in stop_eng and not i in customstopwords])\n",
    "    return rev_new\n",
    "\n",
    "# remove short words (length < 3)\n",
    "dataset['commentText'] = dataset['commentText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# remove stopwords from the text\n",
    "reviews = [remove_stopwords(r.split()) for r in dataset['commentText']]\n",
    "\n",
    "# make entire text lowercase\n",
    "reviews = [r.lower() for r in reviews]\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#tokenized_word=word_tokenize(text)\n",
    "#print(tokenized_word)\n",
    "\n",
    "\n",
    "stop_eng = stopwords.words('english')\n",
    "\n",
    "\n",
    "tokens = []\n",
    "sentences = []\n",
    "tokenizedSentences =[]\n",
    "filtered_sent=[]\n",
    "\n",
    "for i in tqdm(range(0,len(dataset))):\n",
    "    for txt in (dataset['Description']):\n",
    "        sentences.append(txt.lower())\n",
    "        tokenized=word_tokenize(txt)\n",
    "        #tokenized = [t.lower().strip(\":,.!?\") for t in sentences.split()]   #.encode('utf-8')\n",
    "        #iltered_tokens.append(function(tokens)) = [w for w in tokens if not w in stop_eng and not w in customstopwords and w.isalpha() and not len(w)<3 and not w in hashtags and not w in ghashtags and not w in links and not w in mentions]\n",
    "        tokens.extend(tokenized)\n",
    "        tokenizedSentences.append(tokenized)\n",
    "        #print(filtered_tokens)\n",
    "\n",
    "#for w in tokenizedSentences:\n",
    " #   if w not in stop_eng:\n",
    "  #      filtered_sent.append(w)\n",
    "        \n",
    "\n",
    "hashtags = [w for w in tokens if w.startswith('#')]\n",
    "ghashtags = [w for w in tokens if w.startswith('+')]\n",
    "mentions = [w for w in tokens if w.startswith('@')]\n",
    "links = [w for w in tokens if w.startswith('http') or w.startswith('www')]\n",
    "#filtered_tokens = [w for w in tokens if not w in stop_eng and not w in customstopwords and w.isalpha() and not len(w)<3 and not w in hashtags and not w in ghashtags and not w in links and not w in mentions]\n",
    "\n",
    "#fd = FreqDist(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, tags=['NOUN', 'ADJ']): # filter noun and adjective\n",
    "       output = []\n",
    "       for sent in texts:\n",
    "        doc=nlp(sent)\n",
    "        #output.append([sent.split()])\n",
    "        output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "             #doc = nlp(sent.split())\n",
    "             #output.append([nlp(sent.split())])\n",
    "             #output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_2 = lemmatization(reviews)\n",
    "print(reviews_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(reviews_2)\n",
    "corpus = [dictionary.doc2bow(text) for text in reviews_2]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model0.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
